{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1me_TmoIpgdS"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0qwbStPTpgdT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryTz6TdCpgdU",
        "outputId": "904a2b42-4ea7-45b0-922d-1041ac9afdae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: translated_sentences.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating a csv-file with two columns (sentence and label)\n",
        "file_path = 'TRAINING_DATA.txt'\n",
        "\n",
        "# Initializing lists to store sentences and labels\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "# Reading the text file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        # Split the line based on tab or other delimiter used between label and sentence\n",
        "        label, sentence = line.strip().split('\\t')  # Adjust delimiter if different\n",
        "        sentences.append(sentence)\n",
        "        labels.append(int(label))  # Convert label to integer\n",
        "\n",
        "# Creating a DataFrame with two columns: 'Sentence' and 'Label'\n",
        "data = pd.DataFrame({\n",
        "    'Sentence': sentences,\n",
        "    'Label': labels\n",
        "})\n",
        "\n",
        "# Saving the DataFrame to a CSV file\n",
        "csv_path = \"translated_sentences.csv\"\n",
        "data.to_csv(csv_path, index=False, encoding='utf-8')\n",
        "\n",
        "print(f\"CSV file created at: {csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "3tLxKsT5pgdV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "f10a3d48-8b98-4355-a8cb-39e0abc2ffe4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Sentence\n",
              "Label                                                   \n",
              "1      Cuando conocí a Janice en 2013 , una familia n...\n",
              "0      Hwang habló en Sur de este año por Southwest M...\n",
              "1      Usted podría pensar Katy Perry y Robert Pattin...\n",
              "1      Cualquiera que haya volado los cielos del crea...\n",
              "1      Bueno , este cantante tendrá un LARGO tiempo p...\n",
              "...                                                  ...\n",
              "0      \" Ella repetía , como hemos luchado durante ve...\n",
              "1      Y despues de vender sus acciones en Polar Bear...\n",
              "0                      No es sólo malo para los pobres .\n",
              "0      Mientras espera la quinta oleada aún más letal...\n",
              "1      Mañana vveremos entrevistas de Marissa Mayer ,...\n",
              "\n",
              "[17877 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9d3726b-9ab8-4198-8aca-9f849d905f25\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sentence</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Label</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cuando conocí a Janice en 2013 , una familia n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Hwang habló en Sur de este año por Southwest M...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Usted podría pensar Katy Perry y Robert Pattin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cualquiera que haya volado los cielos del crea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Bueno , este cantante tendrá un LARGO tiempo p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" Ella repetía , como hemos luchado durante ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Y despues de vender sus acciones en Polar Bear...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>No es sólo malo para los pobres .</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mientras espera la quinta oleada aún más letal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Mañana vveremos entrevistas de Marissa Mayer ,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>17877 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9d3726b-9ab8-4198-8aca-9f849d905f25')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9d3726b-9ab8-4198-8aca-9f849d905f25 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9d3726b-9ab8-4198-8aca-9f849d905f25');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-987cfdbc-ba0d-4577-a9c7-5025cac8fa99\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-987cfdbc-ba0d-4577-a9c7-5025cac8fa99')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-987cfdbc-ba0d-4577-a9c7-5025cac8fa99 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 17877,\n  \"fields\": [\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 17877,\n        \"samples\": [\n          \"Por qu\\u00e9 no probar su vagina como \\u00e9ste de 19 a\\u00f1os , chica de Tennessee hizo !\",\n          \"Tuve que poner por escrito el libro , levantarse y caminar alrededor de la habitaci\\u00f3n despu\\u00e9s de ese p\\u00e1rrafo .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df = pd.read_csv('translated_sentences.csv')\n",
        "df.set_index('Label',drop=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ji69AWYApgdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85dc3ffe-8318-4b5e-fb66-27ec59e98493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(17877, 2)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)\n",
        "data.fillna(\"\",inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DiK0jiKOpgdV"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P7b1Lh0DpgdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95720194-5048-4b95-8d23-0ae82731fa71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "print(nltk.data.path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EVTMdrm6pgdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af177340-9712-433f-9d60-2ebc388431f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Download stopwords to the appropriate directory\n",
        "import nltk\n",
        "nltk.download('stopwords', download_dir='nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0sHdJ4rvpgdV"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    # Keep only alphabetic characters and whitespace\n",
        "    cleaned_text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage:\n",
        "data = pd.read_csv(\"translated_sentences.csv\")  # Assuming the CSV is created as per previous instructions\n",
        "data['Cleaned_Sentence'] = data['Sentence'].apply(remove_special_characters)\n",
        "\n",
        "# Save the cleaned version\n",
        "data.to_csv(\"cleaned_sentences.csv\", index=False, encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "adCBmpahpgdW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "d08997bd-27ab-490e-883e-61335dcf7990"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.clean_text(text)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>clean_text</b><br/>def clean_text(text)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-9-0b5e7b10bc5b&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation using a regular expression\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Keep only letters and whitespace\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra whitespace\n",
        "    return cleaned_text\n",
        "\n",
        "clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f1wF9lDPpgdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4564ab24-9add-40be-df29-a4d2abbbf3c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Download required NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JcrM5270pgdW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.data.path.append('/nltk_data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "WOlGo68ypgdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb6220e-7d35-4b23-95e4-514ed8585f63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vRW2d1YipgdW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32273a6d-d826-4218-eb67-cc65eb66e808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hola', 'cómo', 'bien', 'gracias', 'preguntar']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure necessary resources are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation using a regular expression\n",
        "    cleaned_text = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚñÑ\\s]', '', text)  # Keep only letters, accented vowels, and whitespace\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()  # Remove extra whitespace\n",
        "    return cleaned_text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize text using the Spanish language option\n",
        "    tokens = word_tokenize(text, language='spanish')\n",
        "\n",
        "    # Load Spanish stopwords\n",
        "    stop_words = set(stopwords.words('spanish'))\n",
        "\n",
        "    # Remove stopwords\n",
        "    cleaned_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    return cleaned_tokens\n",
        "\n",
        "# Example usage\n",
        "text = \"Hola, ¿cómo estás? Estoy bien, gracias por preguntar.\"\n",
        "cleaned_text = clean_text(text)  # Clean the text first\n",
        "cleaned_tokens = preprocess_text(cleaned_text)  # Then preprocess the cleaned text\n",
        "print(cleaned_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_spanish(tokens):\n",
        "    lemmatized = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
        "    return lemmatized\n",
        "\n",
        "# Example usage\n",
        "lemmatized_tokens = lemmatize_spanish(cleaned_tokens)\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "UgV1FmwVrUcj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73433911-ae88-47e7-f627-53586b926841"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hola', 'cómo', 'bien', 'gracias', 'preguntar']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textstat"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiK2oB2NQ-kn",
        "outputId": "12198cec-b0ea-45c7-95d6-cc11b5d96c49"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textstat\n",
            "  Downloading textstat-0.7.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pyphen (from textstat)\n",
            "  Downloading pyphen-0.16.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from textstat) (75.1.0)\n",
            "Downloading textstat-0.7.4-py3-none-any.whl (105 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/105.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.1/105.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyphen-0.16.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyphen, textstat\n",
            "Successfully installed pyphen-0.16.0 textstat-0.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "from textstat import textstat\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Load spacy model for POS tagging and Named Entity Recognition\n",
        "nlp = spacy.load(\"en_core_web_sm\")  # Choose appropriate model for the target language\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def calculate_lexical_diversity(text):\n",
        "    words = [word for word in word_tokenize(text) if word.isalpha() and word not in stop_words]\n",
        "    return len(set(words)) / len(words) if len(words) > 0 else 0\n",
        "\n",
        "def calculate_readability_scores(text):\n",
        "    return textstat.flesch_reading_ease(text)\n",
        "\n",
        "# Example function for extracting lexical features\n",
        "def extract_lexical_features(texts):\n",
        "    lexical_features = []\n",
        "    for text in texts:\n",
        "        lexical_diversity = calculate_lexical_diversity(text)\n",
        "        readability_score = calculate_readability_scores(text)\n",
        "        lexical_features.append([lexical_diversity, readability_score])\n",
        "\n",
        "    return np.array(lexical_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4f8JfUve3MM",
        "outputId": "537ede90-462d-44e3-d145-4464f76b16a4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_pos_distribution(text):\n",
        "    doc = nlp(text)\n",
        "    pos_counts = Counter([token.pos_ for token in doc])\n",
        "    total_tokens = sum(pos_counts.values())\n",
        "    return {pos: count / total_tokens for pos, count in pos_counts.items()}\n",
        "\n",
        "def extract_pos_features(texts):\n",
        "    pos_features = []\n",
        "    for text in texts:\n",
        "        pos_distribution = calculate_pos_distribution(text)\n",
        "        # Selecting only some POS tags for analysis (e.g., 'NOUN', 'VERB', 'ADJ', etc.)\n",
        "        selected_pos = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON']\n",
        "        features = [pos_distribution.get(pos, 0) for pos in selected_pos]\n",
        "        pos_features.append(features)\n",
        "\n",
        "    return np.array(pos_features)"
      ],
      "metadata": {
        "id": "zNiyI8_de6VQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return len(named_entities)\n",
        "\n",
        "def extract_named_entity_features(texts):\n",
        "    ner_features = [count_named_entities(text) for text in texts]\n",
        "    return np.array(ner_features).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "9tijhPlae85z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cbevuUwSnhE",
        "outputId": "7ffea486-18d0-4687-c05d-c61540d82938"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.8.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (24.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.66.5)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (0.44.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2024.8.30)\n",
            "Downloading language_tool_python-2.8.1-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import language_tool_python\n",
        "\n",
        "tool = language_tool_python.LanguageTool('en-US')  # Change to the target language\n",
        "\n",
        "def count_grammar_errors(text):\n",
        "    matches = tool.check(text)\n",
        "    return len(matches)\n",
        "\n",
        "def extract_grammar_error_features(texts):\n",
        "    grammar_features = [count_grammar_errors(text) for text in texts]\n",
        "    return np.array(grammar_features).reshape(-1, 1)"
      ],
      "metadata": {
        "id": "5dDDCp7Je_c4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load a BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def encode_text_bert(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"tf\", truncation=True, padding=\"max_length\", max_length=128)\n",
        "    outputs = bert_model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Extract the [CLS] token representation\n",
        "\n",
        "def calculate_semantic_similarity(text, native_corpus):\n",
        "    text_vector = encode_text_bert(text)\n",
        "    corpus_vectors = np.vstack([encode_text_bert(t) for t in native_corpus])\n",
        "    similarities = cosine_similarity(text_vector, corpus_vectors)\n",
        "    return np.mean(similarities)\n",
        "\n",
        "# Example function for extracting semantic similarity features\n",
        "def extract_semantic_features(texts, native_corpus):\n",
        "    semantic_features = [calculate_semantic_similarity(text, native_corpus) for text in texts]\n",
        "    return np.array(semantic_features).reshape(-1, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ph-gvGUdfDgK",
        "outputId": "302198ae-4434-4a83-c493-e16a062d51f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_all_features(texts, native_corpus):\n",
        "    lexical_features = extract_lexical_features(texts)\n",
        "    pos_features = extract_pos_features(texts)\n",
        "    ner_features = extract_named_entity_features(texts)\n",
        "    grammar_features = extract_grammar_error_features(texts)\n",
        "    semantic_features = extract_semantic_features(texts, native_corpus)\n",
        "\n",
        "    # Combine all features into a single feature matrix\n",
        "    combined_features = np.hstack((lexical_features, pos_features, ner_features, grammar_features, semantic_features))\n",
        "    return combined_features"
      ],
      "metadata": {
        "id": "soZL2BxqfGjz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "CNIJWmPy8L-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def extract_tfidf_features(sentences):\n",
        "    # TF-IDF with unigrams and bigrams\n",
        "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)  # Adjust max_features as needed\n",
        "    tfidf_features = tfidf_vectorizer.fit_transform(sentences)\n",
        "    return tfidf_features\n",
        "\n",
        "# Example usage\n",
        "tfidf_features = extract_tfidf_features(data['Sentence'])"
      ],
      "metadata": {
        "id": "pX4RYig1sTlL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "\n",
        "def extract_pos_tags(sentences):\n",
        "    pos_counts = []\n",
        "    for sentence in sentences:\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "        # Calculating the count of each POS type ('NN' for noun, 'VB' for verb, 'JJ' for adjective)\n",
        "        pos_counts.append({pos: sum(1 for word, tag in pos_tags if tag == pos) for pos in ['NN', 'VB', 'JJ']})\n",
        "    return pos_counts\n",
        "\n",
        "\n",
        "pos_features = extract_pos_tags(data['Sentence'])"
      ],
      "metadata": {
        "id": "z6kpK9pgsW2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b6b588-b623-4fa2-9ea0-f3a023ba6f25"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_length_features(sentences):\n",
        "    length_features = []\n",
        "    for sentence in sentences:\n",
        "        num_words = len(sentence.split())\n",
        "        num_chars = len(sentence)\n",
        "        length_features.append({'word_count': num_words, 'char_count': num_chars})\n",
        "    return length_features\n",
        "\n",
        "# Example usage\n",
        "length_features = extract_length_features(data['Sentence'])"
      ],
      "metadata": {
        "id": "okJ_ErXIswJA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def load_glove_model(file_path):\n",
        "    return KeyedVectors.load_word2vec_format(file_path, binary=False)\n",
        "\n",
        "def get_average_word_vectors(sentences, model, num_features=300):\n",
        "    # Assuming model is pre-trained with 300 dimensions\n",
        "    features = []\n",
        "    for sentence in sentences:\n",
        "        words = sentence.split()\n",
        "        word_vectors = [model[word] for word in words if word in model]\n",
        "        if word_vectors:\n",
        "            features.append(np.mean(word_vectors, axis=0))\n",
        "        else:\n",
        "            features.append(np.zeros(num_features))\n",
        "    return np.array(features)\n",
        "\n"
      ],
      "metadata": {
        "id": "Dpnga_MVsyM2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textstat import flesch_reading_ease\n",
        "\n",
        "def extract_readability_features(sentences):\n",
        "    readability_scores = [flesch_reading_ease(sentence) for sentence in sentences]\n",
        "    return readability_scores\n",
        "\n",
        "# Example usage\n",
        "readability_features = extract_readability_features(data['Sentence'])"
      ],
      "metadata": {
        "id": "dcmlQQ8us36v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "length_df = pd.DataFrame(length_features)\n",
        "pos_df = pd.DataFrame(pos_features)\n",
        "readability_df = pd.DataFrame(readability_features, columns=['readability'])\n",
        "\n",
        "# Combining with TF-IDF features using sparse matrix stacking\n",
        "final_features = hstack([tfidf_features, length_df, pos_df, readability_df])\n"
      ],
      "metadata": {
        "id": "XYoRtchps77d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train-test-split"
      ],
      "metadata": {
        "id": "qvm6P7oP8YRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# Step 1: Extract readability and POS features\n",
        "readability_features = extract_readability_features(data['Sentence'])\n",
        "pos_features = extract_pos_tags(data['Sentence'])\n",
        "\n",
        "# Step 2: Create a new DataFrame for combined features\n",
        "features_df = pd.DataFrame({\n",
        "    'Readability_Score': readability_features,\n",
        "    'NN_Count': [feat.get('NN', 0) for feat in pos_features],\n",
        "    'VB_Count': [feat.get('VB', 0) for feat in pos_features],\n",
        "    'JJ_Count': [feat.get('JJ', 0) for feat in pos_features]\n",
        "})\n",
        "\n",
        "# Combine these features with the labels\n",
        "final_data = pd.concat([features_df, data['Label']], axis=1)\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X = final_data.drop(columns=['Label'])  # Features\n",
        "y = final_data['Label']  # Target\n",
        "\n",
        "# Split the data (80% for training and 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Display shapes of the resulting datasets\n",
        "print(f\"Training set: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"Test set: X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "id": "OIgeVjVftm8I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10086ef-db7d-4578-9391-111f96647c2f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: X_train shape: (14301, 4), y_train shape: (14301,)\n",
            "Test set: X_test shape: (3576, 4), y_test shape: (3576,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "EaSzICSd8qY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load a pre-trained BERT tokenizer and model for sequence classification\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Example text to classify\n",
        "text = \"This is an example sentence.\"\n",
        "\n",
        "# Tokenize the input text\n",
        "inputs = tokenizer(text, return_tensors='tf', truncation=True, padding=True, max_length=128)\n",
        "\n",
        "# Run the model to get predictions\n",
        "outputs = model(inputs)\n",
        "\n",
        "# Convert logits to probabilities using softmax\n",
        "probabilities = tf.nn.softmax(outputs.logits, axis=-1)\n",
        "\n",
        "# Print the predicted class and confidence scores\n",
        "predicted_class = tf.argmax(probabilities, axis=-1).numpy()\n",
        "confidence_scores = probabilities.numpy()\n",
        "\n",
        "print(f\"Predicted Class: {predicted_class[0]}, Confidence Scores: {confidence_scores}\")"
      ],
      "metadata": {
        "id": "aC8xHpHAB2n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textstat"
      ],
      "metadata": {
        "id": "8mjbxki8IlHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.data import Dataset\n",
        "import textstat\n",
        "import nltk\n",
        "\n",
        "# Ensure NLTK dependencies are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Step 1: Load and clean training data\n",
        "file_path = 'translated_sentences.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "data.fillna(\"\", inplace=True)\n",
        "\n",
        "def remove_special_characters(text):\n",
        "    return re.sub(r'[^A-Za-z\\s]', '', text)\n",
        "\n",
        "# Clean sentences\n",
        "data['Cleaned_Sentence'] = data['Sentence'].apply(remove_special_characters)\n",
        "\n",
        "def extract_pos_tags(sentences):\n",
        "    pos_counts = []\n",
        "    for sentence in sentences:\n",
        "        tokens = nltk.word_tokenize(sentence)\n",
        "        pos_tags = nltk.pos_tag(tokens)\n",
        "        pos_counts.append({pos: sum(1 for word, tag in pos_tags if tag == pos) for pos in ['NN', 'VB', 'JJ']})\n",
        "    return pos_counts\n",
        "\n",
        "def extract_readability_features(sentences):\n",
        "    return [textstat.flesch_reading_ease(sentence) for sentence in sentences]\n",
        "\n",
        "# Extract features\n",
        "readability_features = extract_readability_features(data['Sentence'])\n",
        "pos_features = extract_pos_tags(data['Sentence'])\n",
        "\n",
        "# Create features DataFrame\n",
        "features_df = pd.DataFrame({\n",
        "    'Readability_Score': readability_features,\n",
        "    'NN_Count': [feat.get('NN', 0) for feat in pos_features],\n",
        "    'VB_Count': [feat.get('VB', 0) for feat in pos_features],\n",
        "    'JJ_Count': [feat.get('JJ', 0) for feat in pos_features]\n",
        "})\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_df, data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Load BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "bert_model = TFBertModel.from_pretrained(model_name)\n",
        "\n",
        "# Create BERT embeddings in batches\n",
        "def encode_text_bert_batch(texts, tokenizer, model, batch_size=8):\n",
        "    texts = texts.tolist()\n",
        "    dataset = Dataset.from_tensor_slices(texts).batch(batch_size)\n",
        "    bert_embeddings = []\n",
        "\n",
        "    for batch in dataset:\n",
        "        batch_text = [str(text) for text in batch.numpy()]\n",
        "        inputs = tokenizer(batch_text, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "        outputs = model(inputs)\n",
        "        cls_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "        bert_embeddings.append(cls_embeddings)\n",
        "\n",
        "    return np.vstack(bert_embeddings)\n",
        "\n",
        "# Generate BERT embeddings for training and testing sets\n",
        "bert_embeddings_train = encode_text_bert_batch(data.loc[X_train.index, 'Sentence'], tokenizer, bert_model)\n",
        "bert_embeddings_test = encode_text_bert_batch(data.loc[X_test.index, 'Sentence'], tokenizer, bert_model)\n",
        "\n",
        "# Combine BERT embeddings with other features\n",
        "final_train_features = np.hstack([bert_embeddings_train, X_train.values])\n",
        "final_test_features = np.hstack([bert_embeddings_test, X_test.values])\n",
        "\n",
        "# Train a Logistic Regression model\n",
        "model = LogisticRegression(max_iter=20000)\n",
        "model.fit(final_train_features, y_train)\n",
        "\n",
        "# Evaluate model\n",
        "predicted_labels = model.predict(final_test_features)\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "precision = precision_score(y_test, predicted_labels)\n",
        "recall = recall_score(y_test, predicted_labels)\n",
        "f1 = f1_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.6f}\")\n",
        "print(f\"Precision: {precision:.6f}\")\n",
        "print(f\"Recall: {recall:.6f}\")\n",
        "print(f\"F1 Score: {f1:.6f}\")\n",
        "\n",
        "# Step 2: Process real data\n",
        "real_data_file_path = 'REAL_DATA.txt'\n",
        "lines = []\n",
        "\n",
        "with open(real_data_file_path, 'r', encoding='utf-8') as file:\n",
        "    for line_number, line in enumerate(file, start=1):\n",
        "        columns = line.split('\\t')\n",
        "        if len(columns) == 2:\n",
        "            lines.append(columns)\n",
        "        else:\n",
        "            print(f\"Skipping or correcting line {line_number}: {line.strip()}\")\n",
        "\n",
        "real_data = pd.DataFrame(lines, columns=['Label', 'Sentence'])\n",
        "real_data['Label'] = real_data['Label'].astype(int)\n",
        "real_data['Cleaned_Sentence'] = real_data['Sentence'].apply(remove_special_characters)\n",
        "\n",
        "# Extract features for real data\n",
        "readability_features_real = extract_readability_features(real_data['Sentence'])\n",
        "pos_features_real = extract_pos_tags(real_data['Sentence'])\n",
        "\n",
        "features_real_df = pd.DataFrame({\n",
        "    'Readability_Score': readability_features_real,\n",
        "    'NN_Count': [feat.get('NN', 0) for feat in pos_features_real],\n",
        "    'VB_Count': [feat.get('VB', 0) for feat in pos_features_real],\n",
        "    'JJ_Count': [feat.get('JJ', 0) for feat in pos_features_real]\n",
        "})\n",
        "\n",
        "# Generate BERT embeddings for real data\n",
        "bert_embeddings_real = encode_text_bert_batch(real_data['Sentence'], tokenizer, bert_model)\n",
        "\n",
        "# Combine BERT embeddings with other features for real data\n",
        "final_real_features = np.hstack([bert_embeddings_real, features_real_df.values])\n",
        "\n",
        "# Make predictions for real data\n",
        "predicted_real_labels = model.predict(final_real_features)\n",
        "\n",
        "# Replace '2' in the Label column with predictions\n",
        "real_data['Label'] = predicted_real_labels\n",
        "\n",
        "# Save modified real data\n",
        "real_data.to_csv('modified_REAL_DATA.txt', sep='\\t', index=False, header=False)\n",
        "\n",
        "print(\"Real data file updated with predictions and saved as 'modified_REAL_DATA.txt'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AgoCZlbVs2Q",
        "outputId": "039b3388-3ff4-45c6-e951-1135e359c793"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.529642\n",
            "Precision: 0.540787\n",
            "Recall: 0.519452\n",
            "F1 Score: 0.529905\n",
            "Real data file updated with predictions and saved as 'modified_REAL_DATA.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate the model\n",
        "predicted_labels = model.predict(final_test_features)\n",
        "\n",
        "# Calculate and print evaluation metrics\n",
        "accuracy = accuracy_score(y_test, predicted_labels)\n",
        "precision = precision_score(y_test, predicted_labels)\n",
        "recall = recall_score(y_test, predicted_labels)\n",
        "f1 = f1_score(y_test, predicted_labels)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.3f}\")\n",
        "print(f\"Precision: {precision:.3f}\")\n",
        "print(f\"Recall: {recall:.3f}\")\n",
        "print(f\"F1 Score: {f1:.3f}\")"
      ],
      "metadata": {
        "id": "qOpXVKaIsHxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load Real Data\n",
        "real_data_file_path = '/mnt/data/REAL_DATA.txt'  # Adjust the path as needed\n",
        "real_data = pd.read_csv(real_data_file_path, sep='\\t', header=None, names=['Label', 'Sentence'])\n",
        "\n",
        "# Step 2: Clean and preprocess sentences (if needed)\n",
        "real_data['Cleaned_Sentence'] = real_data['Sentence'].apply(remove_special_characters)\n",
        "\n",
        "# Step 3: Extract features for real data (same as training data)\n",
        "readability_features_real = extract_readability_features(real_data['Sentence'])\n",
        "pos_features_real = extract_pos_tags(real_data['Sentence'])\n",
        "\n",
        "# Create feature DataFrame for real data\n",
        "features_real_df = pd.DataFrame({\n",
        "    'Readability_Score': readability_features_real,\n",
        "    'NN_Count': [feat.get('NN', 0) for feat in pos_features_real],\n",
        "    'VB_Count': [feat.get('VB', 0) for feat in pos_features_real],\n",
        "    'JJ_Count': [feat.get('JJ', 0) for feat in pos_features_real]\n",
        "})\n",
        "\n",
        "# Step 4: Generate BERT embeddings for real data\n",
        "bert_embeddings_real = encode_text_bert_batch(real_data['Sentence'], tokenizer, bert_model, batch_size=8)\n",
        "\n",
        "# Step 5: Combine BERT embeddings with other features for real data\n",
        "final_real_features = np.hstack([bert_embeddings_real, features_real_df.values])\n",
        "\n",
        "# Step 6: Make predictions for real data\n",
        "predicted_real_labels = model.predict(final_real_features)\n",
        "\n",
        "# Step 7: Replace '2' in the Label column with predictions\n",
        "real_data['Label'] = predicted_real_labels\n",
        "\n",
        "# Save the modified real data to a new file\n",
        "real_data.to_csv('modified_REAL_DATA.txt', sep='\\t', index=False, header=False)\n",
        "\n",
        "print(\"Real data file updated with predictions and saved as 'modified_REAL_DATA.txt'.\")"
      ],
      "metadata": {
        "id": "uhLkMiTssJ0X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}